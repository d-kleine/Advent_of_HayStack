{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/d-kleine/Advent_of_HayStack/blob/main/Advent_of_Haystack_Multi_Query_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q2FHwPY-wqJ"
   },
   "source": [
    "# Advent of Haystack: Day 3\n",
    "\n",
    "_Make a copy of this Colab to start_\n",
    "\n",
    "In this challenge, you must help Elf David build a system to answer questions over BBC news dataset. However, to increase recall, your system should be able to generate questions similar to the ones asked.\n",
    "\n",
    "For instance, if Santa asks, `\"How are cybersecurity threats evolving with new technologies?\"` the system should be able to generate similar questions like:\n",
    "\n",
    "- `\"What impact do emerging technologies like AI and IoT have on the landscape of cybersecurity threats?\"`\n",
    "- `\"How are organizations adapting their cybersecurity strategies in response to the evolution of threats driven by technological advancements?\"`\n",
    "- `\"In what ways are cybercriminals leveraging new technologies to enhance their attack methods and tactics?\"`\n",
    "\n",
    "All these questions are similar to the original question, but they are not the same. The idea is that by generating similar questions, you can increase the system's recall, as the system will be able to retrieve more documents that could contain the answer to the original question.\n",
    "For that, you will use a large language model (LLM) to generate alternative similar questions based on the original question.\n",
    "Each of these similar questions will query a document store with news articles; all the documents retrieved by each similar question will be used to compose an answer to the original question.\n",
    "\n",
    "\n",
    "### Components to use:\n",
    "\n",
    "- [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore): to store the news articles.\n",
    "- [`InMemoryEmbeddingRetriever`](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever): to retrieve the documents from the document store.\n",
    "- [`OpenAIGenerator`](https://docs.haystack.deepset.ai/docs/openaigenerator): to instantiate the LLM to generate similar questions and compose an answer to the original question.\n",
    "- [`PromptBuilder`](https://docs.haystack.deepset.ai/docs/promptbuilder): to build the prompts to query the LLM\n",
    "- [`AnswerBuilder`](https://docs.haystack.deepset.ai/docs/answerbuilder): (optional) to build the answers to the original question.\n",
    "- [`SentenceTransformersTextEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder): to embed the questions\n",
    "- [`SentenceTransformersDocumentEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder): to embed the news articles\n",
    "- [`DocumentJoiner`](https://docs.haystack.deepset.ai/docs/documentjoiner): to join the documents retrieved by similar query questions\n",
    "\n",
    "### Your task is to build two custom components:\n",
    "\n",
    "- `MultiQueryGenerator`: a custom component that uses an LLM to generate similar questions based on the original question.\n",
    "- `MultiQueryHandler`: a custom component that queries the document store with a set of query questions and collects all the documents\n",
    "\n",
    "**Note:** Feel free to change the models in this challenge and use different model providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZALZDx-LFebK"
   },
   "source": [
    "### 1) Installation\n",
    "\n",
    "Install packages and the BBC news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QJJ5AT3Z79e3"
   },
   "outputs": [],
   "source": [
    "# !pip install haystack-ai\n",
    "# !pip install \"sentence-transformers>=3.0.0\"\n",
    "# !pip install lazy_imports\n",
    "# # !wget https://raw.githubusercontent.com/amankharwal/Website-data/master/bbc-news-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3jwS4A_FmEV"
   },
   "source": [
    "### 2) Enter API keys for LLM\n",
    "If you want to use OpenAI models, save your API key as `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zy4xBS6v8n1s",
    "outputId": "303b3c51-2268-4f20-c292-c7e0f63a77b4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = json.load(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85pAAbaPFtZc"
   },
   "source": [
    "### 3) Parse the news dataset and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oXMwKWzUAEaW"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb7289d79cc48039515f8ce8ef0fa10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 2225}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from typing import List\n",
    "\n",
    "from haystack import Document\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack import Pipeline\n",
    "\n",
    "def read_documents(file: str) -> List[Document]:\n",
    "    with open(file, \"r\") as file:\n",
    "        reader = csv.reader(file, delimiter=\"\\t\")\n",
    "        next(reader, None)  # skip the headers\n",
    "        docs = []\n",
    "        for row in reader:\n",
    "            category = row[0].strip()\n",
    "            title = row[2].strip()\n",
    "            text = row[3].strip()\n",
    "            docs.append(Document(content=text, meta={\"category\": category, \"title\": title}))\n",
    "\n",
    "    return docs\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "doc_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"embedder\", SentenceTransformersDocumentEmbedder(model=embedding_model))\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(doc_store, policy=DuplicatePolicy.OVERWRITE))\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "\n",
    "documents = read_documents(\"bbc-news-data.csv\")\n",
    "indexing_pipeline.run({\"embedder\":{\"documents\": documents}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQbBLYVI1cXZ"
   },
   "source": [
    "### 4) Define a custom component `MultiQueryGenerator`\n",
    "This custom component will generate `n` alternatives to each query using a [Generator](https://docs.haystack.deepset.ai/docs/generators) and a [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder).\n",
    "\n",
    "We have prepared a template for this component, feel free to change it as you wish.\n",
    "\n",
    "**Hint:** You can start with this prompt template 👇  \n",
    "```python\n",
    "template =\n",
    "\"\"\"\n",
    "You are an AI language model assistant. Your task is to generate {{n_variations}} different versions of the\n",
    "given user question by expanding the meaning of it.\n",
    "By generating multiple perspectives on the user question, you will help gather diverse information that will be useful to answer the user question in more comprehensive manner.\n",
    "The generated questions should be concise. Do not just rephrase the question, think about the other topics that are relevent to the user question.\n",
    "\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {{question}}\n",
    "Alternative: questions:\n",
    "\"\"\"\n",
    "```\n",
    "**Hint** If you're using OpenAI models, feel free to use [`n` parameter](https://platform.openai.com/docs/api-reference/chat/create#chat-create-n) of the API and update the prompt accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yMtN6v5n1cXZ"
   },
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "@component\n",
    "class MultiQueryGenerator:\n",
    "    def __init__(self):\n",
    "        # You need to define a Generator and a PromptBuilder to pass to that generator\n",
    "        # The template of the PromptBuilder will have two variables:\n",
    "        #    - 'query' a string,\n",
    "        #    - 'n_variations' the number of variations of the query string to generate\n",
    "        \n",
    "        template = \"\"\"\n",
    "        You are an AI language model assistant. Your task is to generate {{n_variations}} different versions of the given original question \"{{query}}\" by expanding the meaning of it.\n",
    "        By generating multiple perspectives on this original question, you will help gather diverse information that will be useful to answer the original question in more comprehensive manner.\n",
    "        The generated alternative questions should be concise. Do not just rephrase the question, think about the other topics that are relevant to the original question.\n",
    "\n",
    "        Provide these alternative questions to the original question separated by newlines. Print the original question and the derived alternative questions.\n",
    "        Original question: {{query}}\n",
    "        Alternative questions:\n",
    "        \"\"\"\n",
    "        \n",
    "        self.prompt_builder = PromptBuilder(template=template)\n",
    "        self.generator = OpenAIGenerator(model=\"gpt-4o-mini\")\n",
    "        \n",
    "    @component.output_types(queries=List[str])\n",
    "    def run(self, query: str, n_variations: int = 3):\n",
    "        # Build the prompt with input query and number of variations\n",
    "        prompt_data = self.prompt_builder.run(query=query, n_variations=n_variations)\n",
    "        prompt = prompt_data[\"prompt\"]  # Extract the string prompt\n",
    "\n",
    "        # Use the generator to create variations of the query\n",
    "        response = self.generator.run(prompt=prompt)\n",
    "        generated_queries = response[\"replies\"]  # Extract generated responses\n",
    "\n",
    "        # Combine the original query with the generated variations\n",
    "        queries = [query] + generated_queries\n",
    "        return {\"queries\": queries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wyu5dZXH1cXZ"
   },
   "source": [
    "### 5) Define the custom component `MultiQueryHandler`\n",
    "\n",
    "This component will query the document store with multiple questions and collect all the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m5CM7YsO1cXZ"
   },
   "outputs": [],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "\n",
    "@component\n",
    "class MultiQueryHandler:\n",
    "    def __init__(self, document_store, embedding_model: str):\n",
    "        # Initialize your SentenceTransformersTextEmbedder and # here\n",
    "        self.embedder = SentenceTransformersTextEmbedder(embedding_model)\n",
    "        self.embedder.warm_up()\n",
    "        # Initialize the retriever with the document store and embedder\n",
    "        self.retriever = InMemoryEmbeddingRetriever(\n",
    "            document_store=document_store)\n",
    "\n",
    "    @component.output_types(answers=List[Document])\n",
    "    def run(self, queries: List[str], top_k: int = 3):\n",
    "        # Initialize an empty list to store retrieved documents for all queries\n",
    "        documents = []\n",
    "        \n",
    "        # Loop through each query to retrieve relevant documents\n",
    "        for query in queries:\n",
    "            # Generate query embedding using the embedder\n",
    "            query_embedding = self.embedder.run(text=query)\n",
    "            \n",
    "            # Retrieve documents using the embedding\n",
    "            retrieved_docs = self.retriever.run(\n",
    "                query_embedding=query_embedding[\"embedding\"],\n",
    "                top_k=top_k\n",
    "            )\n",
    "            \n",
    "            # Add retrieved documents to the main list\n",
    "            documents.extend(retrieved_docs[\"documents\"])\n",
    "        \n",
    "        return {\"answers\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynhmh2Y-1cXZ"
   },
   "source": [
    "### 6) Define the RAG Pipeline with Multi-Query Retrieval\n",
    "\n",
    "Given a question, this RAG pipeline will generate multiple similar questions, query the document store and collect all the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EuOfXHrP1cXZ",
    "outputId": "2dbbb7c9-825c-41a7-b27c-19faa6537448"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001D52CB97290>\n",
       "🚅 Components\n",
       "  - multi_query_generator: MultiQueryGenerator\n",
       "  - multi_query_handler: MultiQueryHandler\n",
       "  - reranker: DocumentJoiner\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "  - answer_builder: AnswerBuilder\n",
       "🛤️ Connections\n",
       "  - multi_query_generator.queries -> multi_query_handler.queries (List[str])\n",
       "  - multi_query_handler.answers -> reranker.documents (List[Document])\n",
       "  - reranker.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)\n",
       "  - llm.replies -> answer_builder.replies (List[str])\n",
       "  - llm.meta -> answer_builder.meta (List[Dict[str, Any]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder, AnswerBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "template = \"\"\"\n",
    "You have to answer the following question based on the given context information only.\n",
    "If the context is empty or just a '\\\\n' answer with None, example: \"None\".\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# add components\n",
    "pipeline.add_component(\"multi_query_generator\", MultiQueryGenerator())\n",
    "pipeline.add_component(\"multi_query_handler\", MultiQueryHandler(document_store=doc_store,embedding_model=embedding_model))\n",
    "pipeline.add_component(\"reranker\", DocumentJoiner(join_mode=\"reciprocal_rank_fusion\"))\n",
    "pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "pipeline.add_component(\"llm\", OpenAIGenerator())\n",
    "pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
    "\n",
    "# connect components\n",
    "pipeline.connect(\"multi_query_generator.queries\", \"multi_query_handler.queries\")\n",
    "pipeline.connect(\"multi_query_handler.answers\", \"reranker.documents\")\n",
    "pipeline.connect(\"reranker\", \"prompt_builder.documents\")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "pipeline.connect(\"llm.meta\", \"answer_builder.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kvmVQ85X1cXZ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdc023bb7904799ad6656ca3bf15176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e3fa28a6a14fbbaeb3bc62bd9eb7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# question = \"Can you give me some suggestions do you have for Christmas presents? Please provide a variety of options.\"\n",
    "question = \"What is popular in the music industry today?\"\n",
    "# question = \"How are cybersecurity threats evolving with new technologies?\"\n",
    "# question = \"What does UK do to prevent piracy in music industry?\"\n",
    "\n",
    "n_variations = 3\n",
    "top_k = 3\n",
    "\n",
    "result = pipeline.run(\n",
    "    {'multi_query_generator':{'query':question, 'n_variations':n_variations},\n",
    "     'multi_query_handler':{'top_k':top_k},\n",
    "     'prompt_builder': {'template_variables': {'question':question}},\n",
    "     'answer_builder':{'query':question}\n",
    "     }, include_outputs_from={\"multi_query_generator\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-ixCJwuSGny"
   },
   "source": [
    "Print the answer and the queries used to retrieve more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vTgGCbO91cXZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Questions:\n",
      "\n",
      "What is popular in the music industry today?\n",
      "Original question: What is popular in the music industry today?  \n",
      "Alternative questions:  \n",
      "How are streaming platforms shaping current music trends and popular artists?  \n",
      "What genres or characteristics are dominating charts and listeners' preferences right now?  \n",
      "Which emerging artists and collaborations are gaining significant attention in the music scene?  \n",
      "\n",
      "\n",
      "Answer:\n",
      "\n",
      "The internet and digital sales are popular in the music industry today, with musicians embracing online promotion, file-sharing and digital music sales as a way to reach new fans and sell more music.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nQuestions:\\n\")\n",
    "for q in result['multi_query_generator']['queries']:\n",
    "    print(q)\n",
    "print(\"\\n\\nAnswer:\\n\")\n",
    "print(result['answer_builder']['answers'][0].data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
